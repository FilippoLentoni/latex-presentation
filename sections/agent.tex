% =======================
% Agent Evaluation Section
% =======================
\section{Agent Evaluation}

% ---------------- Slide: From LLMs to Agents ----------------
\begin{frame}{From LLMs to Agents}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{LLM Application:}
\begin{itemize}
  \item Single model invocation
  \item Direct input $\rightarrow$ output
  \item Stateless (typically)
  \item Limited to text generation
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Agent:}
\begin{itemize}
  \item Orchestrated system
  \item Multiple LLM calls
  \item Uses tools \& external systems
  \item Multi-step reasoning
  \item Maintains state across turns
\end{itemize}
\end{column}
\end{columns}

\vspace{0.5em}
\begin{center}
\small
\texttt{User} $\rightarrow$ \fbox{LLM$_1$} $\rightarrow$ \fbox{Tool} $\rightarrow$ \fbox{LLM$_2$} $\rightarrow$ \fbox{Action} $\rightarrow$ \texttt{Output}
\end{center}

\begin{block}{Evaluation Implication}
Increased complexity = more failure modes = more evaluation dimensions required.
\end{block}
\end{frame}

% ---------------- Slide: What is an Agent ----------------
\begin{frame}{Anatomy of an LLM Agent}

\begin{block}{Definition}
An \textbf{agent} is a system that uses an LLM as its reasoning engine to:
\begin{enumerate}
  \item Interpret user intent
  \item Plan a sequence of actions
  \item Execute actions using tools
  \item Observe results and iterate
\end{enumerate}
\end{block}

\textbf{Core Components:}
\begin{itemize}
  \item \textbf{Brain}: The LLM
  \item \textbf{Tools}: External capabilities (APIs, databases, code execution)
  \item \textbf{Memory}: Context persistence across interactions
  \item \textbf{Guardrails}: Safety and policy constraints
\end{itemize}

\end{frame}
% ---------------- Slide: Examples of Agents ----------------
\begin{frame}{Examples of Agentic Applications}

\small
\begin{tabular}{p{2.8cm}p{4.2cm}p{3.5cm}}
\toprule
\textbf{Agent Type} & \textbf{Description} & \textbf{Key Evaluation Focus} \\
\midrule
Data Analysis Agent & Query databases, generate reports, create visualizations & Query correctness, data grounding \\
\addlinespace
% Explainability Agent & Analyze code/models, explain inputs/outputs & Reasoning accuracy \\
% \addlinespace
Document Intelligence & Extract, summarize, answer questions from documents & Extraction completeness, citation accuracy \\
\addlinespace
Coding Agent & Generate, review, debug, and execute code & Functional correctness, security \\
\addlinespace
% Orchestrator Agent & Coordinate multiple sub-agents & Action sequencing, delegation accuracy \\
% \addlinespace
% Research Agent & Search, synthesize, and summarize information & Source quality, hallucination rate \\
\bottomrule
\end{tabular}

\end{frame}

% ---------------- Slide: What We Control ----------------
\begin{frame}{What We Control in Agent Systems}

\begin{tabular}{p{3.2cm}p{4cm}p{3cm}}
\toprule
\textbf{Control Point} & \textbf{Description} & \textbf{Optimization Goal} \\
\midrule
Agent Architecture & Number of agents, interaction patterns (sequential, parallel, hierarchical) & Right-size complexity \\
\addlinespace
Model per Agent & Which LLM for each sub-task & Cost/performance balance \\
\addlinespace
Agent Prompts & System instructions per agent & Task accuracy \\
\addlinespace
Tools / MCP & Available actions, APIs, data sources & Capability coverage \\
\bottomrule
\end{tabular}

% \vspace{0.5em}
% \begin{block}{Design Principle}
% Find the \textbf{simplest and cheapest} combination of agents that solves your problem. More agents $\neq$ better.
% \end{block}
\end{frame}

% ---------------- Slide: Why Agent Eval is Hard ----------------
\begin{frame}{Why Agent Evaluation is Challenging}

\textbf{Agents introduce new failure modes beyond single LLM calls:}

\begin{enumerate}
  \item \textbf{Multi-step dependencies}: Error in step 1 cascades to step N
  
  \item \textbf{Tool selection}: Agent may choose wrong tool or wrong parameters
  
  \item \textbf{State management}: Context may be lost or corrupted across turns
  
  \item \textbf{Non-determinism}: Same query can take different paths
  
  \item \textbf{Side effects}: Actions may change external state (databases, APIs)
\end{enumerate}
\end{frame}

% ---------------- Slide: Metrics Taxonomy Overview ----------------
\begin{frame}{AI Metrics Taxonomy for Agents}

\textbf{Hierarchical Relationship:}
$$
\text{Adoption} \xleftarrow{\text{depends on}} \text{Coverage} \xleftarrow{\text{within scope}} \begin{cases} \text{Accuracy} \\ \text{Latency} \\ \text{Cost} \end{cases}
$$

\small
\begin{tabular}{p{2cm}p{5cm}p{3.2cm}}
\toprule
\textbf{Metric} & \textbf{Definition} & \textbf{How Measured} \\
\midrule
\textbf{Adoption} & User engagement with the agent & Unique users, queries/user, return rate, session length \\
\addlinespace
\textbf{Coverage} & Percentage of in-scope queries the agent can handle & \% handled vs. refused or failed \\
\addlinespace
\textbf{Latency} & Time from query to complete response & Time to first token, total time, \# of turns \\
\addlinespace
\textbf{Accuracy} & Correctness, completeness, and safety of responses & Using the Accuracy Bridge taxonomy \\
\addlinespace
\textbf{Cost} & Resource consumption per query & \$/query, tokens used, API calls, compute \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\begin{block}{Trade-offs}
Improving accuracy may increase latency and cost. Optimizing cost may reduce accuracy. Evaluation must consider all dimensions.
\end{block}
\end{frame}

% ---------------- Slide: Coverage Deep Dive ----------------
% \begin{frame}{Coverage: Defining Agent Scope}

% \textbf{Coverage} = the spectrum of queries the agent is designed to handle.

% \begin{block}{Expected Behaviors}
% \begin{itemize}
%   \item \textbf{In-scope query}: Agent should provide accurate, complete response
%   \item \textbf{Out-of-scope query}: Agent should \textit{explicitly refuse}
%   \begin{itemize}
%     \item ``I'm not able to answer this question.''
%     \item ``This is outside my capabilities. Please contact [X].''
%   \end{itemize}
% \end{itemize}
% \end{block}

% \textbf{Coverage Analysis:}
% \begin{enumerate}
%   \item Log all user queries (in-scope and out-of-scope)
%   \item Analyze out-of-scope queries to distinguish:
%   \begin{itemize}
%     \item True misuse (user asking wrong agent)
%     \item Capability gap (should be in scope, but isn't)
%   \end{itemize}
%   \item Use gaps to inform roadmap and expand evaluation dataset
% \end{enumerate}

% \begin{block}{Key Insight}
% Graceful refusal is \textbf{correct behavior}---test for it explicitly.
% \end{block}
% \end{frame}

% ---------------- Slide: Latency Deep Dive ----------------
% \begin{frame}{Latency: Breaking Down Response Time}

% \textbf{Latency} can be decomposed into multiple components:

% \begin{center}
% \small
% \begin{tabular}{p{3.5cm}p{6.5cm}}
% \toprule
% \textbf{Component} & \textbf{Definition} \\
% \midrule
% Time to First Token (TTFT) & Time until agent starts responding \\
% Total Response Time & Time from query to complete response \\
% Per-Tool Latency & Time spent in each tool invocation \\
% Number of Turns & Conversation rounds to reach answer \\
% Planning Time & Time spent in reasoning/orchestration \\
% \bottomrule
% \end{tabular}
% \end{center}

% \begin{block}{Context-Dependent Thresholds}
% Acceptable latency depends on use case:
% \begin{itemize}
%   \item Interactive chat: $<5$ seconds expected
%   \item Complex analysis: Minutes may be acceptable
%   \item Background task: Hours may be fine
% \end{itemize}
% \end{block}

% Define latency thresholds \textbf{per task type} in your evaluation criteria.
% \end{frame}

% ---------------- Slide: Accuracy Bridge Intro ----------------
% \begin{frame}{Accuracy: Beyond a Single Number}

% \textbf{Challenge:} ``The agent was 85\% accurate'' doesn't enable improvement.

% \begin{block}{The Accuracy Bridge}
% A standardized taxonomy that decomposes inaccuracies into:
% \begin{enumerate}
%   \item \textbf{Failure Type}: What went wrong (user-visible outcome)
%   \item \textbf{Root Cause}: Why it went wrong (technical origin)
%   \item \textbf{Example}: Concrete illustration
% \end{enumerate}
% \end{block}

% \textbf{Benefits:}
% \begin{itemize}
%   \item \textbf{Actionable}: Links failure type to fix (prompting? tools? retrieval?)
%   \item \textbf{Comparable}: Same taxonomy across different agents
%   \item \textbf{Trackable}: Monitor if fixes reduce specific failure types over time
%   \item \textbf{Reportable}: Aggregated view for stakeholders
% \end{itemize}
% \end{frame}

% ---------------- Slide: Accuracy Bridge Table 1 ----------------
\begin{frame}{The Accuracy Bridge: Failure Taxonomy}

\scriptsize
\begin{tabular}{p{1.8cm}p{3cm}p{2.8cm}p{2.8cm}}
\toprule
\textbf{Failure Type} & \textbf{Description} & \textbf{Root Causes} & \textbf{Example} \\
\midrule
\textbf{Hallucination} (Factually Wrong) & Information that is incorrect or not grounded in source data & Prompting, Retrieval, Tool error, Model limitation, Data quality & Agent returns incorrect value for a metric \\
\addlinespace
\textbf{Incomplete} & Response omits critical information required to answer & Prompting, Orchestration, Retrieval, Model limitation & missing root causes  \\
\addlinespace
\textbf{Extra Information} & Includes unrequested, irrelevant information & Prompting, Model limitation & Agent adds unrelated suggestions to a factual query \\
\addlinespace
\textbf{Wrong Format} & Content correct, but format doesn't match request & Prompting, Output schema, Model limitation & Returns paragraph when table was requested \\
\bottomrule
\end{tabular}
\end{frame}

% ---------------- Slide: Accuracy Bridge Table 2 ----------------
\begin{frame}{The Accuracy Bridge: Failure Taxonomy}

\scriptsize
\begin{tabular}{p{1.8cm}p{3cm}p{2.8cm}p{2.8cm}}
\toprule
\textbf{Failure Type} & \textbf{Description} & \textbf{Root Causes} & \textbf{Example} \\
\midrule
\textbf{Wrong Action} & Executes incorrect, unintended, or unsafe state-changing action & Orchestration logic, Prompting, Tool config, Missing guardrails & Agent updates wrong record in database \\
\addlinespace
\textbf{Throttling / Timeout} & Fails to complete due to rate or time limits & Infrastructure limits, Retry logic, Token constraints, Tool latency & Agent stops mid-reasoning, returns error \\
\addlinespace
\textbf{Unsafe / Policy Violation} & Response or action violates safety, compliance, or policy & Prompting, Missing guardrails, Tool misuse, Model limitation & Agent exposes restricted data or performs unauthorized action \\
\bottomrule
\end{tabular}

\begin{block}{Feedback Loop}
Accuracy Bridge $\rightarrow$ Root Cause Analysis $\rightarrow$ Targeted Fix $\rightarrow$ Re-evaluate
\end{block}
\end{frame}

% ---------------- Slide: Offline vs Online ----------------
% \begin{frame}{Two Evaluation Modes: Offline and Online}

% \begin{columns}[T]
% \begin{column}{0.48\textwidth}
% \textbf{Offline Evaluation}\\
% (Pre-Deployment)
% \begin{itemize}
%   \item Run agent against curated dataset
%   \item Ground truth available
%   \item Controlled environment
%   \item Gate for deployment
% \end{itemize}
% \end{column}
% \begin{column}{0.48\textwidth}
% \textbf{Online Evaluation}\\
% (Post-Deployment)
% \begin{itemize}
%   \item Monitor production traffic
%   \item Ground truth often unavailable
%   \item Real user behavior
%   \item Continuous monitoring
% \end{itemize}
% \end{column}
% \end{columns}

% \vspace{0.5em}
% \begin{block}{Both Are Required}
% \begin{itemize}
%   \item Offline: Necessary but not sufficient (can't anticipate all real queries)
%   \item Online: Catches drift, edge cases, and unexpected user behavior
% \end{itemize}
% \end{block}
% \end{frame}

% ---------------- Slide: Building Eval Dataset ----------------
\begin{frame}{Building a High-Quality Evaluation Dataset}

\textbf{Best Practices:}

\begin{itemize}
  \item[\checkmark] Define expected output for each input
  \item[\checkmark] Include diverse query types
  \item[\checkmark] Add edge cases and ambiguous inputs
  \item[\checkmark] Include multi-turn conversations
  \item[\checkmark] Test refusal behavior (out-of-scope)
  \item[\checkmark] Use SMEs for ground truth
  \item[\checkmark] Consider cost of mistakes per category
\end{itemize}

\vspace{0.5em}
\begin{block}{Synthetic Data Generation}
Use an LLM to generate diverse test queries based on your agent's scope. Human review is still required for ground truth labels.
\end{block}
\end{frame}

% ---------------- Slide: Version Control ----------------
\begin{frame}{Versioning: Experiments and Datasets}

\begin{block}{What to Version}
\begin{itemize}
  \item \textbf{Agent Configuration}
  \begin{itemize}
    \item Model(s) used, prompts, tool configurations
    \item Orchestration logic, parameters
  \end{itemize}
  \item \textbf{Evaluation Dataset}
  \begin{itemize}
    \item Input queries, expected outputs, labels
    \item Dataset version ID, creation date, author
  \end{itemize}
  \item \textbf{Results}
  \begin{itemize}
    \item Metrics per experiment run
    \item Mapping: Agent version $\times$ Dataset version $\rightarrow$ Metrics
  \end{itemize}
\end{itemize}
\end{block}

\end{frame}

% ---------------- Slide: Online Workflow ----------------
\begin{frame}{Observability and Online Evaluation Workflow}

\textbf{Post-Deployment Monitoring:}

\begin{enumerate}
  \item \textbf{Automatic Telemetry Capture}
  \begin{itemize}
    \item Log all interactions: queries, outputs, tool calls, reasoning
    \item Capture latency per step, total cost, tokens used
    \item Collect user feedback (thumbs up/down, explicit ratings)
  \end{itemize}
  
  \item \textbf{Trace Review} (choose based on volume)
  \begin{itemize}
    \item \textit{Low volume}: Review all traces manually
    \item \textit{Medium volume}: Use LLM-as-a-Judge to flag, then human review
    \item \textit{High volume}: Sample (stratified), then LLM-as-a-Judge + human review
  \end{itemize}
  
  \item \textbf{Label Using Accuracy Bridge}
  \begin{itemize}
    \item Enables root cause analysis
    \item Feeds directly into reporting
  \end{itemize}
  
  \item \textbf{Export to Offline Dataset}
  \begin{itemize}
    \item Labeled production traces become new test cases
  \end{itemize}
\end{enumerate}
\end{frame}



% ---------------- Slide: LLM as a Judge Intro ----------------
\begin{frame}{LLM-as-a-Judge: Scaling Evaluation}

\textbf{Concept:} Use an LLM to evaluate another LLM or agent's outputs.

\begin{block}{Two Operating Modes}
\begin{center}
\begin{tabular}{p{2cm}p{4cm}p{4cm}}
\toprule
\textbf{Mode} & \textbf{Offline (with ground truth)} & \textbf{Online (no ground truth)} \\
\midrule
Input & Query + Agent Output + Expected Output & Query + Agent Output + Trace \\
Task & Score against reference & Flag likely issues \\
Use Case & Automate scoring at scale & Prioritize traces for review \\
\bottomrule
\end{tabular}
\end{center}
\end{block}

\begin{block}{Critical Requirement}
LLM-as-a-Judge must be \textbf{calibrated} against human-labeled data. Human labeling remains the source of truth.
\end{block}
\end{frame}

% ---------------- Slide: Judge Example 1 (No Ground Truth) ----------------
\begin{frame}[fragile]{LLM-as-a-Judge Example: Hallucination Detection (No Ground Truth)}

\scriptsize
\textbf{Use Case:} When ground truth is unavailable, judge evaluates plausibility and factual consistency.

\vspace{0.3em}
\begin{block}{Judge Prompt}
\tiny
\texttt{Evaluate the degree of hallucination in the generation on a continuous scale from 0 to 1.}

\texttt{A generation can be considered to hallucinate (Score: 1) if it does not align with established}
\texttt{knowledge, verifiable data, or logical inference, and often includes elements that are}
\texttt{implausible, misleading, or entirely fictional.}

\vspace{0.2em}
\textbf{Example:}
\begin{itemize}
  \item Query: \textit{Can eating carrots improve your vision?}
  \item Generation: \textit{Yes, eating carrots significantly improves your vision, especially at night...}
  \item Score: 1.0
  \item Reasoning: \textit{Carrots only improve vision under specific circumstances (vitamin A deficiency).}
\end{itemize}

\vspace{0.2em}
\textbf{Input Template:}
\texttt{Query: \{\{query\}\} | Generation: \{\{generation\}\} | Think step by step.}
\end{block}

\begin{block}{Key Insight}
Without ground truth, the judge relies on world knowledge and logical consistency.
\end{block}
\end{frame}

% ---------------- Slide: Judge Example 2 (With Ground Truth) ----------------
\begin{frame}[fragile]{LLM-as-a-Judge Example: Correctness Evaluation (With Ground Truth)}

\scriptsize
\textbf{Use Case:} When ground truth is available, judge compares generation against reference answer.

\vspace{0.3em}
\begin{block}{Judge Prompt}
\tiny
\texttt{Evaluate the correctness of the generation on a continuous scale from 0 to 1.}

\texttt{A generation can be considered correct (Score: 1) if it includes all the key facts from the}
\texttt{ground truth and if every fact presented is factually supported by the ground truth or common sense.}

\vspace{0.2em}
\textbf{Example:}
\begin{itemize}
  \item Query: \textit{Can eating carrots improve your vision?}
  \item Generation: \textit{Yes, eating carrots significantly improves your vision...} (same as before)
  \item Ground Truth: \textit{Carrots contain beta-carotene (vitamin A). Extreme lack causes blindness.}
  \item Score: 0.1
  \item Reasoning: \textit{Generation mentions vision improvement but fails to explain the mechanism or circumstances. Contains misinformation and exaggerations.}
\end{itemize}

\vspace{0.2em}
\textbf{Input Template:}
\texttt{Query: \{\{query\}\} | Generation: \{\{generation\}\} | Ground truth: \{\{ground\_truth\}\}}
\end{block}

\begin{block}{Key Insight}
With ground truth, scoring is more objective and can catch subtle omissions or additions.
\end{block}
\end{frame}

% % ---------------- Slide: Langfuse HLD ----------------
% \begin{frame}{Production Agent Evaluation: A Two-Phase System}

% \scriptsize

% \begin{center}
% \begin{tikzpicture}[
%   node distance=0.6cm,
%   phase/.style={rectangle, draw, thick, rounded corners, minimum width=2.5cm, minimum height=2.2cm, font=\scriptsize, align=center, fill=blue!5},
%   langfuse/.style={rectangle, draw, thick, rounded corners, minimum width=1.8cm, minimum height=2.5cm, font=\tiny, align=left, fill=orange!10},
%   arrow/.style={->, thick}
% ]

% % Phase 1: Offline (Beta)
% \node[phase] (beta) at (0,0) {
%   \textbf{Phase 1}\\
%   \textbf{Offline}\\[0.15cm]
%   \tiny
%   • Iterate\\
%   • Eval dataset\\
%   • Judge scores\\
%   • Gate
% };

% % Langfuse in the center
% \node[langfuse] (langfuse) at (3.5,0) {
%   \textbf{Langfuse}\\[0.05cm]
%   \tiny
%   Traces\\
%   Scores\\
%   Datasets\\
%   Annotations\\
%   Prompts\\
%   Evaluators
% };

% % Phase 2: Online (Prod)
% \node[phase] (prod) at (7,0) {
%   \textbf{Phase 2}\\
%   \textbf{Online}\\[0.15cm]
%   \tiny
%   • Real users\\
%   • Monitor\\
%   • Feedback\\
%   • Review
% };

% % Arrows
% \draw[arrow] (beta.east) -- (langfuse.west);
% \draw[arrow] (langfuse.east) -- (prod.west);
% \draw[arrow, bend left=35] (beta.north) to node[above, font=\tiny] {promote} (prod.north);
% \draw[arrow, bend left=35] (prod.south) to node[below, font=\tiny] {feedback} (beta.south);

% \end{tikzpicture}
% \end{center}

% \vspace{0.4em}
% \begin{columns}[T]
% \begin{column}{0.48\textwidth}
% \textbf{Offline (Beta):}
% \begin{itemize}
%   \item Experiment on eval datasets
%   \item LLM-as-a-Judge automated scoring
%   \item Accuracy gate before promotion
% \end{itemize}
% \end{column}
% \begin{column}{0.48\textwidth}
% \textbf{Online (Production):}
% \begin{itemize}
%   \item Continuous trace monitoring
%   \item User feedback collection
%   \item Weekly human review
% \end{itemize}
% \end{column}
% \end{columns}

% \vspace{0.3em}
% \begin{block}{Key Insight}
% Production failures feed back into the evaluation dataset, creating continuous improvement.
% \end{block}

% \end{frame}

% ---------------- Slide: High-Level Architecture ----------------
\begin{frame}{High-Level Architecture}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/HLD.png}
\end{center}
\end{frame}

% ---------------- Slide: Demo Chatbot ----------------
\begin{frame}{Shopping Assistant: Demo Chatbot}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/lang_0.png}
\end{center}
\end{frame}

% ---------------- Slide: Langfuse Demo 1 ----------------
\begin{frame}{Langfuse Demo: Traces and Datasets}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/lang_1.png}
\end{center}
\end{frame}

% ---------------- Slide: Langfuse Demo 2 ----------------
\begin{frame}{Langfuse Demo: Scoring and Evaluation}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/lang_2.png}
\end{center}
\end{frame}

% ---------------- Slide: Langfuse Demo 3 ----------------
\begin{frame}{Langfuse Demo: Annotations}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/lang_3.png}
\end{center}
\end{frame}

% ---------------- Slide: Langfuse Demo 4 ----------------
\begin{frame}{Langfuse Demo: Prompt Management}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/lang_4.png}
\end{center}
\end{frame}

% ---------------- Slide: Langfuse Demo 5 ----------------
\begin{frame}{Langfuse Demo: Evaluators}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/Lang_5.png}
\end{center}
\end{frame}

% ---------------- Slide: Langfuse Demo 6 ----------------
\begin{frame}{Langfuse Demo: Analytics Dashboard}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/lang_6.png}
\end{center}
\end{frame}

% ---------------- Slide: Langfuse Demo 7 ----------------
\begin{frame}{Langfuse Demo: Monitoring}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/Lang_7.png}
\end{center}
\end{frame}

% ---------------- Slide: Langfuse Demo 8 ----------------
\begin{frame}{Langfuse Demo: Production Insights}
\begin{center}
\includegraphics[width=0.95\textwidth,height=0.85\textheight,keepaspectratio]{images/Lang_8.png}
\end{center}
\end{frame}

% ---------------- Slide: Closing the Loop ----------------
\begin{frame}{Closing the Feedback Loop}

\begin{center}
\textbf{Continuous Improvement Cycle}
\end{center}

$$
\boxed{\text{Offline Eval}} \xrightarrow{\text{pass}} \boxed{\text{Deploy}} \xrightarrow{\text{monitor}} \boxed{\text{Online Eval}}
$$
$$
\uparrow \hspace{6cm} \downarrow
$$
$$
\boxed{\text{Update Dataset}} \xleftarrow{\text{label \& export}} \boxed{\text{Review Traces}}
$$

\vspace{0.5em}
\textbf{Key Activities:}
\begin{enumerate}
  \item Label production failures using Accuracy Bridge
  \item Export labeled traces to offline evaluation dataset (new version)
  \item Test new agent versions against real failure modes
  \item Use for \textbf{non-regression testing}: ensure fixes don't break other cases
\end{enumerate}

\begin{block}{Result}
Over time, evaluation dataset reflects real production challenges $\rightarrow$ more robust pre-deployment testing.
\end{block}
\end{frame}

% ---------------- Slide: Feedback Loop Agent ----------------
\begin{frame}{Advanced: Automated Feedback Analysis}

\textbf{Concept:} Use an agent to analyze evaluation data and suggest improvements.

\begin{block}{Feedback Analysis Agent}
\textbf{Inputs:}
\begin{itemize}
  \item LLM-as-a-Judge scores
  \item Human labels (Accuracy Bridge categories)
  \item User feedback (thumbs up/down, comments)
  \item Agent traces and codebase structure
\end{itemize}

\textbf{Outputs:}
\begin{itemize}
  \item Pattern detection across failures (common root causes)
  \item Suggested prompt refinements
  \item Identified capability gaps
  \item Prioritized improvement opportunities
\end{itemize}
\end{block}

\end{frame}

% ---------------- Slide: Reporting ----------------
\begin{frame}{Standardized Reporting}

\textbf{Example Status Report Structure:}

\begin{quote}
\small
``As of [date], Agent X can answer \textbf{X\%} of in-scope questions (Coverage). \textbf{X users} submitted \textbf{X queries} this period, $\pm$X\% vs.\ prior (Adoption). Average latency was \textbf{X seconds} over \textbf{X turns} (Latency). Total cost was \textbf{\$X} (Cost). Of \textbf{X reviewed interactions}, \textbf{Y} were inaccurate (Accuracy):
\begin{itemize}
  \item X cases: hallucination (root cause: retrieval)
  \item X cases: wrong action (root cause: tool config)
  \item X cases: timeout (root cause: infrastructure)
\end{itemize}
Corrective actions: ...''
\end{quote}

\begin{block}{Benefits of Standardization}
\begin{itemize}
  \item Comparable across agents
  \item Directly derived from Accuracy Bridge labels
  \item Enables trend tracking over time
\end{itemize}
\end{block}
\end{frame}

% ---------------- Slide: Summary ----------------
\begin{frame}{Agent Evaluation: Key Takeaways}

\begin{enumerate}
  \item \textbf{Agents are complex systems} with more failure modes than single LLM calls
  
  \item \textbf{Multi-dimensional metrics}: Adoption, Coverage, Latency, Accuracy, Cost
  
  \item \textbf{Accuracy Bridge}: Standardized failure taxonomy enables actionable insights
  \begin{itemize}
    \item Failure Type $\rightarrow$ Root Cause $\rightarrow$ Targeted Fix
  \end{itemize}
  
  \item \textbf{Offline + Online}: Both required for comprehensive evaluation
  
  \item \textbf{LLM-as-a-Judge}: Scales evaluation but must be calibrated; humans remain source of truth
  
  \item \textbf{Close the loop}: Production failures $\rightarrow$ evaluation dataset $\rightarrow$ better agents
  
  \item \textbf{Observability is foundational}: Can't improve what you can't trace
  
  \item \textbf{Versioning}: Agent configs, datasets, and results
\end{enumerate}
\end{frame}

% % ---------------- Slide: Comparison ----------------
% \begin{frame}{LLM vs. Agent Evaluation: Summary Comparison}

% \small
% \begin{tabular}{p{3cm}p{4cm}p{4cm}}
% \toprule
% \textbf{Dimension} & \textbf{LLM Application} & \textbf{Agent} \\
% \midrule
% Complexity & Single model call & Multi-step, multi-component \\
% Failure Modes & Output quality & + Tool selection, orchestration, state \\
% Key Metrics & Task-specific (precision, recall, BLEU, etc.) & Adoption, Coverage, Latency, Accuracy, Cost \\
% Accuracy Analysis & Binary or rubric-based & Accuracy Bridge taxonomy \\
% Ground Truth & Often available offline & Harder to define for multi-step tasks \\
% Observability & Input/output logging & Full trace: planning, tools, intermediate steps \\
% Iteration Unit & Prompt version & Agent config (prompts + tools + orchestration) \\
% \bottomrule
% \end{tabular}

% \vspace{0.5em}
% \begin{block}{Unifying Principle}
% Both require: systematic evaluation, continuous monitoring, feedback loops, and human oversight.
% \end{block}
% \end{frame}

% ---------------- Slide: Questions ----------------
\begin{frame}{Questions?}

% \begin{block}{Key Resources}
% \begin{itemize}
%   \item Langfuse Documentation: \url{https://langfuse.com/docs}
%   \item LangChain Evaluation Guide: \url{https://python.langchain.com/docs/guides/evaluation}
%   \item ``Judging LLM-as-a-Judge'' (Zheng et al., 2023)
%   \item RAGAS: \url{https://docs.ragas.io}
% \end{itemize}
% \end{block}

\vspace{1em}
\begin{center}
\Large
\textbf{Thank you!}
\end{center}
\end{frame}
